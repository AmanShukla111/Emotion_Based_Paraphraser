{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install -q datasets evaluate transformers rouge-score nltk","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:47:44.720903Z","iopub.execute_input":"2024-02-28T04:47:44.721183Z","iopub.status.idle":"2024-02-28T04:48:02.382842Z","shell.execute_reply.started":"2024-02-28T04:47:44.721135Z","shell.execute_reply":"2024-02-28T04:48:02.381597Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import DatasetDict, Dataset\nimport transformers \n\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:02.385226Z","iopub.execute_input":"2024-02-28T04:48:02.385523Z","iopub.status.idle":"2024-02-28T04:48:12.703615Z","shell.execute_reply.started":"2024-02-28T04:48:02.385496Z","shell.execute_reply":"2024-02-28T04:48:12.702680Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"4.36.2\n","output_type":"stream"}]},{"cell_type":"code","source":"model_checkpoint = \"t5-small\"","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:12.704954Z","iopub.execute_input":"2024-02-28T04:48:12.705599Z","iopub.status.idle":"2024-02-28T04:48:12.711308Z","shell.execute_reply.started":"2024-02-28T04:48:12.705562Z","shell.execute_reply":"2024-02-28T04:48:12.710576Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from evaluate import load\nmetric = load(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:12.712364Z","iopub.execute_input":"2024-02-28T04:48:12.712629Z","iopub.status.idle":"2024-02-28T04:48:32.851941Z","shell.execute_reply.started":"2024-02-28T04:48:12.712606Z","shell.execute_reply":"2024-02-28T04:48:32.851192Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30df4f0525c34e849ff2693964724866"}},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv('https://raw.githubusercontent.com/KAILASHVenkat/Paraphrasing_model/main/filtered_data.csv')\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:32.854850Z","iopub.execute_input":"2024-02-28T04:48:32.855566Z","iopub.status.idle":"2024-02-28T04:48:33.227960Z","shell.execute_reply.started":"2024-02-28T04:48:32.855527Z","shell.execute_reply":"2024-02-28T04:48:33.227089Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(9389, 2)"},"metadata":{}}]},{"cell_type":"code","source":"max_length_input_text = df['input_text'].str.len().max()\nmax_length_target_text = df['target_text'].str.len().max()\nprint(max_length_input_text)\nprint(max_length_target_text)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:33.228924Z","iopub.execute_input":"2024-02-28T04:48:33.229197Z","iopub.status.idle":"2024-02-28T04:48:33.253757Z","shell.execute_reply.started":"2024-02-28T04:48:33.229172Z","shell.execute_reply":"2024-02-28T04:48:33.252906Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"642\n573\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data, temp_data = train_test_split(df, test_size=0.2, random_state=42)\n\n# Step 2: Split the temp data into validation and test sets (50% validation, 50% test)\nvalidation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n\ntrain_data.reset_index(drop=True, inplace=True)\nvalidation_data.reset_index(drop=True, inplace=True)\ntest_data.reset_index(drop=True, inplace=True)\n\n# Step 3: Save the datasets under the variable name raw_datasets\nraw_datasets = DatasetDict({\n    'train': Dataset.from_pandas(train_data[['input_text', 'target_text']]),\n    'validation': Dataset.from_pandas(validation_data[['input_text', 'target_text']]),\n    'test': Dataset.from_pandas(test_data[['input_text', 'target_text']])\n})","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:33.254871Z","iopub.execute_input":"2024-02-28T04:48:33.255126Z","iopub.status.idle":"2024-02-28T04:48:33.304643Z","shell.execute_reply.started":"2024-02-28T04:48:33.255103Z","shell.execute_reply":"2024-02-28T04:48:33.303830Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"raw_datasets","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:33.305600Z","iopub.execute_input":"2024-02-28T04:48:33.305865Z","iopub.status.idle":"2024-02-28T04:48:33.311468Z","shell.execute_reply.started":"2024-02-28T04:48:33.305841Z","shell.execute_reply":"2024-02-28T04:48:33.310627Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_text', 'target_text'],\n        num_rows: 7511\n    })\n    validation: Dataset({\n        features: ['input_text', 'target_text'],\n        num_rows: 939\n    })\n    test: Dataset({\n        features: ['input_text', 'target_text'],\n        num_rows: 939\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"import datasets\nimport random\nimport pandas as pd\nfrom IPython.display import display, HTML\n\ndef show_random_elements(dataset, num_examples=5):\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n    picks = []\n    for _ in range(num_examples):\n        pick = random.randint(0, len(dataset)-1)\n        while pick in picks:\n            pick = random.randint(0, len(dataset)-1)\n        picks.append(pick)\n    \n    df = pd.DataFrame(dataset[picks])\n    for column, typ in dataset.features.items():\n        if isinstance(typ, datasets.ClassLabel):\n            df[column] = df[column].transform(lambda i: typ.names[i])\n    display(HTML(df.to_html()))","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:33.312603Z","iopub.execute_input":"2024-02-28T04:48:33.312857Z","iopub.status.idle":"2024-02-28T04:48:33.324451Z","shell.execute_reply.started":"2024-02-28T04:48:33.312835Z","shell.execute_reply":"2024-02-28T04:48:33.323736Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"show_random_elements(raw_datasets[\"train\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:33.325509Z","iopub.execute_input":"2024-02-28T04:48:33.325761Z","iopub.status.idle":"2024-02-28T04:48:33.354262Z","shell.execute_reply.started":"2024-02-28T04:48:33.325738Z","shell.execute_reply":"2024-02-28T04:48:33.353415Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_text</th>\n      <th>target_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>admiration to approval: However, the forward has also received praise from outside the Stadio Renzo Barbera.</td>\n      <td>Yet, the forward has also been recognized positively by those outside the Stadio Renzo Barbera.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gratitude to approval: The success of the Ofeq program has made Israel one of seven countries capable of launching such satellites.</td>\n      <td>The success of the Ofeq program has made Israel one of seven countries capable of launching such satellites.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>confusion to curiosity: Are most conspiracy theories themselves conspiracies?</td>\n      <td>Are all theories about conspiracies conspiracy theories?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>confusion to curiosity: Why does keyboard keys are random and not in alphabetical order?</td>\n      <td>Why aren't the letters on the keyboard in alphabetical order?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>confusion to curiosity: Why it is necessary to do MBA after engineering?</td>\n      <td>Is it good to pursue MBA after engineering?</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"metric","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:33.355376Z","iopub.execute_input":"2024-02-28T04:48:33.355634Z","iopub.status.idle":"2024-02-28T04:48:33.364031Z","shell.execute_reply.started":"2024-02-28T04:48:33.355610Z","shell.execute_reply":"2024-02-28T04:48:33.363214Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\nCalculates average rouge scores for a list of hypotheses and references\nArgs:\n    predictions: list of predictions to score. Each prediction\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\n    rouge_types: A list of rouge types to calculate.\n        Valid names:\n        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n        `\"rougeL\"`: Longest common subsequence based scoring.\n        `\"rougeLsum\"`: rougeLsum splits text using `\"\n\"`.\n        See details in https://github.com/huggingface/datasets/issues/617\n    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n    use_aggregator: Return aggregates if this is set to True\nReturns:\n    rouge1: rouge_1 (f1),\n    rouge2: rouge_2 (f1),\n    rougeL: rouge_l (f1),\n    rougeLsum: rouge_lsum (f1)\nExamples:\n\n    >>> rouge = evaluate.load('rouge')\n    >>> predictions = [\"hello there\", \"general kenobi\"]\n    >>> references = [\"hello there\", \"general kenobi\"]\n    >>> results = rouge.compute(predictions=predictions, references=references)\n    >>> print(results)\n    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n\"\"\", stored examples: 0)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n    \ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:33.364918Z","iopub.execute_input":"2024-02-28T04:48:33.365207Z","iopub.status.idle":"2024-02-28T04:48:34.926721Z","shell.execute_reply.started":"2024-02-28T04:48:33.365178Z","shell.execute_reply":"2024-02-28T04:48:34.925693Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8830385a2dae41d7bcea85e53a1eda1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09191237bbeb4c32afa65ca59b490c62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da5564231cc649a3a4f217d6427558dc"}},"metadata":{}}]},{"cell_type":"code","source":"if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n    prefix = \"paraphrase: \"\nelse:\n    prefix = \"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:34.928392Z","iopub.execute_input":"2024-02-28T04:48:34.928744Z","iopub.status.idle":"2024-02-28T04:48:34.933402Z","shell.execute_reply.started":"2024-02-28T04:48:34.928710Z","shell.execute_reply":"2024-02-28T04:48:34.932556Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"max_input_length = 650\nmax_target_length = 580\n\ndef preprocess_function(examples):\n    inputs = [doc for doc in examples[\"input_text\"]]\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n    # Setup the tokenizer for targets\n    labels = tokenizer(text_target=examples[\"target_text\"], max_length=max_target_length, truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:34.937023Z","iopub.execute_input":"2024-02-28T04:48:34.937313Z","iopub.status.idle":"2024-02-28T04:48:34.946434Z","shell.execute_reply.started":"2024-02-28T04:48:34.937288Z","shell.execute_reply":"2024-02-28T04:48:34.945519Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"preprocess_function(raw_datasets['train'][:2])","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:34.947468Z","iopub.execute_input":"2024-02-28T04:48:34.947735Z","iopub.status.idle":"2024-02-28T04:48:34.962440Z","shell.execute_reply.started":"2024-02-28T04:48:34.947711Z","shell.execute_reply":"2024-02-28T04:48:34.961537Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[17142, 12, 18967, 10, 571, 186, 1440, 33, 132, 16, 8, 296, 58, 1333, 55, 1], [3922, 12, 5142, 10, 12433, 11895, 29, 31, 7, 4806, 6, 15364, 6, 243, 160, 384, 47, 10693, 28, 2089, 31, 7, 23173, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[571, 186, 1440, 33, 132, 58, 1], [2150, 12, 15364, 6, 12433, 11895, 29, 31, 7, 4806, 6, 2089, 31, 7, 23173, 1940, 248, 5044, 12, 70, 384, 5, 1]]}"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:34.963460Z","iopub.execute_input":"2024-02-28T04:48:34.963722Z","iopub.status.idle":"2024-02-28T04:48:35.990110Z","shell.execute_reply.started":"2024-02-28T04:48:34.963699Z","shell.execute_reply":"2024-02-28T04:48:35.989195Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36d9db5aadad4609898ea7f2139da8df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5896ffdca884b26b0559ca64655331e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c3250e5e8274fa9bb9e058d763f509b"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:35.991259Z","iopub.execute_input":"2024-02-28T04:48:35.991551Z","iopub.status.idle":"2024-02-28T04:48:38.207352Z","shell.execute_reply.started":"2024-02-28T04:48:35.991525Z","shell.execute_reply":"2024-02-28T04:48:38.206444Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75a0c9ff8c5b47f2ac6cb44c6bc71c3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50dcf30cb2b2497ba0846f5649027b2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff57cac4d5bb490298af3258a313141b"}},"metadata":{}}]},{"cell_type":"code","source":"batch_size = 8\nmodel_name = model_checkpoint.split(\"/\")[-1]\nargs = Seq2SeqTrainingArguments(\n    f\"{model_name}-finetuned-T5\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=7e-3,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=5,\n    predict_with_generate=True,\n    fp16=True,\n    push_to_hub=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:38.208589Z","iopub.execute_input":"2024-02-28T04:48:38.208874Z","iopub.status.idle":"2024-02-28T04:48:38.270544Z","shell.execute_reply.started":"2024-02-28T04:48:38.208848Z","shell.execute_reply":"2024-02-28T04:48:38.269623Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:38.271715Z","iopub.execute_input":"2024-02-28T04:48:38.273386Z","iopub.status.idle":"2024-02-28T04:48:38.277509Z","shell.execute_reply.started":"2024-02-28T04:48:38.273357Z","shell.execute_reply":"2024-02-28T04:48:38.276650Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport numpy as np\nfrom nltk.translate.bleu_score import SmoothingFunction, sentence_bleu, corpus_bleu\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Rouge expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    # Calculate BLEU score\n    smoothing = SmoothingFunction().method1\n    bleu_score = corpus_bleu([[ref.split()] for ref in decoded_labels], [pred.split() for pred in decoded_preds], smoothing_function=smoothing)\n    \n    # Calculate Exact Sentence-level Recall (Exact SR) and Exact F1 (Exact FE)\n    exact_sr = sum([1 for label, pred in zip(decoded_labels, decoded_preds) if label == pred]) / len(decoded_labels)\n    exact_fe = 2 * (exact_sr * bleu_score) / (exact_sr + bleu_score) if (exact_sr + bleu_score) > 0 else 0.0\n\n    # ROUGE scores (existing code)\n    rouge_output = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n    rouge_scores = {key: value * 100 for key, value in rouge_output.items()}\n\n    # Add mean generated length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    gen_len = np.mean(prediction_lens)\n\n    result = {\n        \"gen_len\": gen_len,\n        \"bleu\": bleu_score * 100,\n        \"exact_sr\": exact_sr * 100,\n        \"exact_fe\": exact_fe * 100,\n        **rouge_scores,\n    }\n\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:38.279864Z","iopub.execute_input":"2024-02-28T04:48:38.280575Z","iopub.status.idle":"2024-02-28T04:48:38.292194Z","shell.execute_reply.started":"2024-02-28T04:48:38.280548Z","shell.execute_reply":"2024-02-28T04:48:38.291361Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\nmodel,\nargs,\ntrain_dataset=tokenized_datasets[\"train\"],\neval_dataset=tokenized_datasets[\"validation\"],\ndata_collator=data_collator,\ntokenizer=tokenizer,\ncompute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:38.293267Z","iopub.execute_input":"2024-02-28T04:48:38.293524Z","iopub.status.idle":"2024-02-28T04:48:39.629534Z","shell.execute_reply.started":"2024-02-28T04:48:38.293500Z","shell.execute_reply":"2024-02-28T04:48:39.628802Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:48:39.630578Z","iopub.execute_input":"2024-02-28T04:48:39.630847Z","iopub.status.idle":"2024-02-28T04:57:47.294477Z","shell.execute_reply.started":"2024-02-28T04:48:39.630822Z","shell.execute_reply":"2024-02-28T04:57:47.293579Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240228_044926-a0iwhbmr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/goodboys69/huggingface/runs/a0iwhbmr' target=\"_blank\">daily-flower-17</a></strong> to <a href='https://wandb.ai/goodboys69/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/goodboys69/huggingface' target=\"_blank\">https://wandb.ai/goodboys69/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/goodboys69/huggingface/runs/a0iwhbmr' target=\"_blank\">https://wandb.ai/goodboys69/huggingface/runs/a0iwhbmr</a>"},"metadata":{}},{"name":"stderr","text":"You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2350' max='2350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2350/2350 07:47, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Gen Len</th>\n      <th>Bleu</th>\n      <th>Exact Sr</th>\n      <th>Exact Fe</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.787308</td>\n      <td>14.022400</td>\n      <td>25.218400</td>\n      <td>5.111800</td>\n      <td>8.500600</td>\n      <td>57.731300</td>\n      <td>36.644800</td>\n      <td>54.022700</td>\n      <td>54.221900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.040800</td>\n      <td>1.638021</td>\n      <td>14.156500</td>\n      <td>25.866200</td>\n      <td>5.324800</td>\n      <td>8.831600</td>\n      <td>57.918700</td>\n      <td>37.220500</td>\n      <td>54.158300</td>\n      <td>54.337500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.556500</td>\n      <td>1.541077</td>\n      <td>14.237500</td>\n      <td>26.055700</td>\n      <td>5.537800</td>\n      <td>9.134200</td>\n      <td>58.116100</td>\n      <td>37.159200</td>\n      <td>54.115600</td>\n      <td>54.269700</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.190200</td>\n      <td>1.524991</td>\n      <td>14.434500</td>\n      <td>26.764100</td>\n      <td>5.111800</td>\n      <td>8.584100</td>\n      <td>59.070000</td>\n      <td>38.634800</td>\n      <td>55.192400</td>\n      <td>55.397900</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.859400</td>\n      <td>1.586140</td>\n      <td>14.524000</td>\n      <td>27.715800</td>\n      <td>6.070300</td>\n      <td>9.959300</td>\n      <td>60.062800</td>\n      <td>39.695400</td>\n      <td>56.010800</td>\n      <td>56.204100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2350, training_loss=1.297681358824385, metrics={'train_runtime': 547.3652, 'train_samples_per_second': 68.611, 'train_steps_per_second': 4.293, 'total_flos': 522769867800576.0, 'train_loss': 1.297681358824385, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:57:47.295934Z","iopub.execute_input":"2024-02-28T04:57:47.296291Z","iopub.status.idle":"2024-02-28T04:57:47.301847Z","shell.execute_reply.started":"2024-02-28T04:57:47.296257Z","shell.execute_reply":"2024-02-28T04:57:47.300742Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Assuming tokenized_datasets[\"test\"] contains your test dataset\nsample_input = tokenized_datasets[\"test\"][5]\n\n# Tokenize the input\ntokenized_input = tokenizer(sample_input[\"input_text\"], return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n\n# Move input tensors to the GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenized_input = {key: value.to(device) for key, value in tokenized_input.items()}\n\n# Generate output\nwith torch.no_grad():\n    generated_output = model.generate(\n        **tokenized_input,\n        max_length=400,  # Set the desired maximum length\n        num_beams=4,     # You can adjust the number of beams for diverse outputs\n    )\n\n# Postprocess the Output\ndecoded_output = tokenizer.batch_decode(generated_output, skip_special_tokens=True)[0]\n\n# Print input text and generated output\nprint(\"Input Text:\")\nprint(sample_input[\"input_text\"])\nprint(\"\\nGenerated Output:\")\nprint(decoded_output)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:58:45.585674Z","iopub.execute_input":"2024-02-28T04:58:45.586042Z","iopub.status.idle":"2024-02-28T04:58:46.226675Z","shell.execute_reply.started":"2024-02-28T04:58:45.586011Z","shell.execute_reply":"2024-02-28T04:58:46.225529Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Input Text:\nconfusion to curiosity: How do I know if a girl likes me back or not?\n\nGenerated Output:\nHow do I know if this girl likes me?\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"path/to/save/kaggle/working/\")\ntokenizer.save_pretrained(\"path/to/save/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:58:59.255398Z","iopub.execute_input":"2024-02-28T04:58:59.256081Z","iopub.status.idle":"2024-02-28T04:58:59.766731Z","shell.execute_reply.started":"2024-02-28T04:58:59.256046Z","shell.execute_reply":"2024-02-28T04:58:59.765687Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"('path/to/save/kaggle/working/tokenizer_config.json',\n 'path/to/save/kaggle/working/special_tokens_map.json',\n 'path/to/save/kaggle/working/spiece.model',\n 'path/to/save/kaggle/working/added_tokens.json',\n 'path/to/save/kaggle/working/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Load the saved model and tokenizer\nloaded_model = AutoModelForSeq2SeqLM.from_pretrained(\"path/to/save/kaggle/working/\")\nloaded_tokenizer = AutoTokenizer.from_pretrained(\"path/to/save/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2024-02-28T04:57:47.319908Z","iopub.status.idle":"2024-02-28T04:57:47.320417Z","shell.execute_reply.started":"2024-02-28T04:57:47.320125Z","shell.execute_reply":"2024-02-28T04:57:47.320147Z"},"trusted":true},"execution_count":null,"outputs":[]}]}